#!/bin/bash

#PBS -N {{ runid }}
#PBS -j oe
#PBS -l walltime={{ pbs_walltime }}
#PBS -m abe
##PBS -M eric.winter@jhuapl.edu

#PBS -q {{ pbs_queue }}
#PBS -l select={{ pbs_select }}:ncpus={{ pbs_ncpus }}:mpiprocs=2:ompthreads={{ pbs_ompthreads }}:model=bro

##PBS -A {{ pbs_account }}
##PBS -q regular
##PBS -l select={{ pbs_select }}:ncpus=36:mpiprocs=2:ompthreads=18

echo "Job $PBS_JOBID started at `date` on `hostname`."

# Specify the ID string for the run. This can be set to any desired string.
# PBS_JOBNAME is used here as an example, as it is set by the #PBS -N
# directive near the top of this file.
export RUNID=$PBS_JOBNAME

# Load the required modules for MPI kaiju.
# NOTE: pleiades and electra do not have the "module restore" facility found
# on cheyenne. The modules must be explicitly loaded each time they are
# needed.
# NOTE: This set of modules assumes your kaiju installation was built using
# this same list of modules. If you used different modules at build time (for
# example, if you used a GNU compiler), update this list to use the modules
# from your build-time environment.

# Comment out the module lines for all systems except the one you are using.
# The default system is pleiades, using the Intel compiler.
module purge

# For pleiades/electra:
module load pkgsrc/2021Q2
module load comp-intel/2020.4.304
module load mpi-hpe/mpt.2.23
module load hdf5/1.8.18_mpt

# For cheyenne:
# module load ncarenv/1.3
# module load intel/19.1.1
# module load impi/2019.7.217
# module load ncarcompilers/0.5.0  # Must be after intel
# module load hdf5-mpi/1.10.8
# module load cmake/3.18.2

echo "The following modules are loaded:"
module list

# Define the kaiju installation location.
# NOTE: You MUST set this variable to the path to your kaiju directory, which
# is the top-level directory created when you cloned the kaiju repository.
export KAIJU_INSTALL_DIR=$HOME/kaiju

# Set kaiju-related environment variables.
# This script sets KAIJUHOME and other environment variables.
source $KAIJU_INSTALL_DIR/scripts/setupEnvironment.sh

# Add the kaiju binary directory to the command path.
# NOTE: You should set this variable to the path to the bin subdirectory of
# your kaiju build directory. The setting below assumes that the MPI version
# of kaiju was built in the build_mpi subdirectory of the kaiju home directory
# (which is typically the same as KAIJU_INSTALL_DIR).
export PATH=$KAIJUHOME/build_mpi/bin:$PATH

# Set the MPI_TYPE_DEPTH to 32.
# If this is not done, gamera_mpi.x will crash with a stack traceback that
# includes an error messge like this:
# ...
# MPT ERROR: The program attempted to construct a derived datatype with
# depth 15, but the maximum allowed depth is 14. You can increase...
# ...
# If you see error messages like this at run time, try increasing the value
# assigned to MPI_TYPE_DEPTH in the line below.
export MPI_TYPE_DEPTH=32

# Set the OMP stack size to prevent a crash.
# If this setting is ignored, the model may cause the MPI kaiju code to crash
# with a segmentation fault and core dump. The value of "100M" was chosen
# ~arbitrarily; experimentation may allow a smaller value to be used.
export OMP_STACKSIZE=100M

echo "The active environment variables are:"
printenv

# Run the model. Direct output from the program is saved in a text file.
EXE=gamhelio_mpi.x
echo "Running $EXE on model $RUNID."
# The omplace tool is used to ensure efficient pinning of MPI ranks and OMP
# threads to appropriate sockets and cores. If you omit omplace, your job
# will still run, but it probably be an order of magnitude slower than it
# would be when using omplace.
mpiexec omplace $EXE $RUNID.xml >& ${EXE}.${RUNID}.out

echo "Job $PBS_JOBID ended at `date` on `hostname`."
