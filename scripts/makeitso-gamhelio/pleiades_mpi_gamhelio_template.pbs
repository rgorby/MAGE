#!/bin/bash

#PBS -N {{ sim_runid }}
#PBS -q {{ pbs_queue }}
#PBS -l walltime={{ pbs_walltime }}
#PBS -l select={{ pbs_select }}:ncpus={{ pbs_ncpus }}:mpiprocs={{ pbs_mpiprocs }}:ompthreads={{ pbs_ompthreads }}:model=bro
#PBS -m abe
#PBS -j oe

echo "Job $PBS_JOBID started at `date` on `hostname`."

# Specify the ID string for the run. This can be set to any desired string.
# PBS_JOBNAME is used here as an example, as it is set by the #PBS -N
# directive near the top of this file.
export RUNID=$PBS_JOBNAME

# Load the required modules for MPI kaiju.
module purge
module load pkgsrc/2021Q2
module load comp-intel/2020.4.304
module load mpi-hpe/mpt.2.23
module load hdf5/1.8.18_mpt

echo "The following modules are loaded:"
module list

# Define the kaiju installation location.
# NOTE: You MUST set this variable to the path to your kaiju directory, which
# is the top-level directory created when you cloned the kaiju repository.
export KAIJU_INSTALL_DIR={{ kaiju_home }}

# Set kaiju-related environment variables.
# This script sets KAIJUHOME and other environment variables.
source $KAIJU_INSTALL_DIR/scripts/setupEnvironment.sh

# Add the kaiju binary directory to the command path.
# NOTE: You should set this variable to the path to the bin subdirectory of
# your kaiju build directory. The setting below assumes that the MPI version
# of kaiju was built in the build_mpi subdirectory of the kaiju home directory
# (which is typically the same as KAIJU_INSTALL_DIR).
export PATH={{ kaiju_build_bin }}:$PATH

# Set the MPI_TYPE_DEPTH to 32.
# If this is not done, gamera_mpi.x will crash with a stack traceback that
# includes an error messge like this:
# ...
# MPT ERROR: The program attempted to construct a derived datatype with
# depth 15, but the maximum allowed depth is 14. You can increase...
# ...
# If you see error messages like this at run time, try increasing the value
# assigned to MPI_TYPE_DEPTH in the line below.
export MPI_TYPE_DEPTH=32

# Set the OMP stack size to prevent a crash.
# If this setting is ignored, the model may cause the MPI kaiju code to crash
# with a segmentation fault and core dump. The value of "100M" was chosen
# ~arbitrarily; experimentation may allow a smaller value to be used.
export OMP_STACKSIZE=100M

echo "The active environment variables are:"
printenv

# Run the model. Direct output from the program is saved in a text file.
EXE=gamhelio_mpi.x
echo "Running $EXE on model $RUNID."
# The omplace tool is used to ensure efficient pinning of MPI ranks and OMP
# threads to appropriate sockets and cores. If you omit omplace, your job
# will still run, but it probably be an order of magnitude slower than it
# would be when using omplace.
mpiexec omplace $EXE $RUNID.xml >& ${EXE}.${RUNID}.out

echo "Job $PBS_JOBID ended at `date` on `hostname`."
